%!TEX root = ./thesis.tex

\chapter{Introduction}
\label{cha:intro}

This work focus on the implementation of \glspl{spn} in \gls{fpga} using low precision numbers. Two numbers representation are studied: Posit representation, also known as type III \gls{unum} \cite{posit_std} and floating point representation \cite{float_std}. Posit is a quite new standard (2017) and has a lot of advantages compared to floating point such has an increased range of numbers and an increased precision for a large range of values. However, there are some disadvantages to posit, such as the increased hardware size, that justify the fact that it is not a good idea to completely replace floating point by posit for general operations. In this work, we focus on the use of posit for specific use case of \glspl{spn} and try to find out in what cases posit would be preferable to floating point.

Chapter \ref{cha:soa} begins with theory about \gls{spn}. \Glspl{spn} are a new architecture \cite{spns} of deep networks for graphical probabilistic inference. This architecture is designed to make the partition function tractable. Tractable means that it is possible to retrieve the impact of each input feature of the network on the output. Then, low precision arithmetic, namely posit \cite{posit_std} and floating point \cite{float_std} standard are described. Posit and floating point are two number representations as binary string for computer based arithmetic. While floating point is widely used on all electronic devices up to now, posit is a new standard which has a lot of advantages compared to floating point and may be more efficient in a lot of applications. Therefore, this work studies the use of posit in \gls{spn}.

In Chapter \ref{cha:eb}, a model is developed to compute an error bound for posit representation based on the error bound for floating point in \cite{errorbound_float}. Number representation used in this work does not conforms to standard of posit and floating point respectively defined in \cite{posit_std} and \cite{float_std}. Each number representation uses a custom definition of the fields length to minimize the error bound. This error bound represents an upper bound for any set of inputs of the \gls{spn}. As in \cite{errorbound_float}, relative errors are used since values inside a \gls{spn} may be very small and not be efficiently represented by standard number representation such as double floating point.

Then, the development of an hardware implementation of \gls{spn} using custom posit representation is done in Chapter \ref{cha:hard}. It uses a trained \gls{spn} as input and shows how to convert it up to \gls{hdl}. This process is automatized, only some meta parameters should be set to generate a new \gls{spn}. An architecture integrating the \gls{spn} alongside with a control unit and memory is built. Thanks to this architecture, the \gls{spn} can be controlled.

Finally, in Chapter \ref{cha:res} , the results from both Chapter \ref{cha:eb} and \ref{cha:hard} are shown. First the error bound for a large set of \gls{spn} is analyzed. Then, the hardware implementation of a specific \gls{spn} is analyzed and demonstrate the operation of the full design.
