%!TEX root = ./thesis.tex

\chapter{Error bound}
\label{cha:eb}

In this chapter we described the models of error bound for both posit and floating point representation. An error bound is an estimation of the worst case scenario. That means that there are no errors acutally reported in real cases that are bigger than the computed error bound. All errors are computed in absolute value.

% ==============================================================================
\section{Error bound for Float}
% ==============================================================================
As float is a widely used format, there already exists error bounds for floating point representation in the context of \glspl{spn} \cite{errorbound_float}. It uses a relative error bound instead of an absolute error because absolute error for floating point can be very small and cannot be represented by a standard floating point number while a relative error can be represented using floatng point numbers.

In \cite{errorbound_float}, relative error is defined as follow: Given a number $f$, the encoded using a floating point representation is $\hat{f}$ with an absolute error of $\Delta f$ and a relative error of $\epsilon$ as described in Equation \ref{eq:float_error}.

\begin{equation}
	\tilde{f} = f \pm \Delta f = f \cdot \left(1 \pm \frac{\Delta f}{f} \right) = f \cdot (1 \pm \epsilon)
	\label{eq:float_error}
\end{equation}

For \glspl{spn} applications, errors can occurs in three different cases: While encoding a new number for any litterals or any results of an addition or multiplication, while performing an addition and while performing a multiplication. Note that there is a difference between the error made while performing and encoding an operation.

% ------------------------------------------------------------------------------
\subsection{Encoding error}
% ------------------------------------------------------------------------------

The relative error for a floating point number can be computed based on the fraction size ($fs$) as it is done is Equation \ref{eq:float_err_enc}. As the size of floating point fields are constant, this error is a constant for any value encoded. The only exception we take into account is zero which can be encoded with a relative error equals to zero.

\begin{equation}
	|\epsilon| = \left| \frac{\Delta f}{f} \right| \leq 2^{-(fs+1)} = 2^{-(N-es+1)}
	\label{eq:float_err_enc}
\end{equation}

% ------------------------------------------------------------------------------
\subsection{Addition error}
% ------------------------------------------------------------------------------
Given $\epsilon$, the error of encoding a floating point number, a number $a$ with an accumulated relative error of $\epsilon_a$ and a number $b$ with an accumulated relative error $\epsilon_b$. A new error bound for the result of the addition of $a$ and $b$ can be computed through Equation \ref{eq:float_add_err}.

\begin{align}
\begin{split}
\hat{f} &= (\hat{a} + \hat{b}) \cdot (1 \pm \epsilon)\\
		&= (a \cdot (1 \pm \epsilon_a) + b \cdot (1 \pm \epsilon_b))\cdot (1 \pm \epsilon)\\
		&= (a + b + a \epsilon_a + b \epsilon_b) \cdot (1 \pm \epsilon) \\
		&= (a + b) \cdot \left(1 \pm \frac{a}{a + b} \cdot \epsilon_a \pm \frac{b}{a+b} \cdot \epsilon_b \right) \cdot (1 \pm \epsilon) \\
		&\leq (a+b) \cdot (1 \pm max(\epsilon_a, \epsilon_b)) \cdot (1 \pm \epsilon)\\
(1+\epsilon_f) &= (1 \pm max(\epsilon_a, \epsilon_b)) \cdot (1 \pm \epsilon)
\end{split}
\label{eq:float_add_err}
\end{align}

% ------------------------------------------------------------------------------
\subsection{Multiplication error}
% ------------------------------------------------------------------------------
Given $\epsilon$, the error of encoding a floating point number, a number $a$ with an accumulated relative error of $\epsilon_a$ and a number $b$ with an accumulated relative error $\epsilon_b$. A new error bound for the result of the addition of $a$ and $b$ can be computed through Equation \ref{eq:float_mult_err}.

\begin{align}
\begin{split}
\hat{f} &= \hat{a} \cdot \hat{b} \cdot (1 \pm \epsilon)\\
		&= a \cdot (1 \pm \epsilon_a) \cdot b \cdot (1 \pm \epsilon_b)\\
		&= a \cdot b \cdot (1 \pm \epsilon_a) \cdot (1 \pm \epsilon_b) \cdot (1 \pm \epsilon)\\
(1 \pm \epsilon_f) &= (1 \pm \epsilon_a) \cdot (1 \pm \epsilon_b) \cdot (1 \pm \epsilon)
\end{split}
\label{eq:float_mult_err}
\end{align}

% ==============================================================================
\section{Error bound for Posit}
% ==============================================================================
In this section, it is considered that a posit number has $N$ bits (without sign bit), $rs$ is the regime size (variable), $es$ is the number of exponent bits (fixed) and $ms$ is the number of mantissa bits.

As shown in Equation \ref{eq:rel_err}, the relative error for a posit representation can be expressed using the same method as for floating point (Eq. \ref{eq:float_err_enc}). Again, it is more interesting to compute the relative error since posit representation has a wider range than floating point representation. Therefore an absolute error would not be encodable as a floating point numbers.

\begin{equation}
	\hat{p} = p \pm \Delta p = p \cdot \left(1 \pm \frac{\Delta p}{p}\right) = p \cdot (1 \pm \epsilon)
	\label{eq:rel_err}
\end{equation}

Note that there also exists some reasearch about absolute error for posit representation as it is done in \cite{posit_arithmetic} or \cite{bfp}. These require to use a model with a high precision for very small errors.

% ------------------------------------------------------------------------------
\subsection{Encoding error}
% ------------------------------------------------------------------------------

In the same way as it is done for floating point in Equation \ref{eq:float_err_enc}, the encoding error can be computed on the basis of the fraction size $fs$. For posit representation, the fraction size is not constant and depends on the value of the encoded number.

Given a number which is in the bound of the posit representation, the maximum relative error that could occur is expressed by Equation \ref{eq:posit_enc_error}. This error represent the inability to encode one more fraction bit. The fraction size is represented by $fs$, $rs$ represents the regime size and $es$ the exponent size. $N$ is the number of bits of the posit representation.

\begin{equation}
	|\epsilon| = \left |\frac{\Delta p}{p} \right| \leq 2^{-(fs+1)} = 2^{-(N-min(rs+es, N)+1)}
	\label{eq:posit_enc_error}
\end{equation}


In contrast to floating point, this error highly depends on the number to be represented. It would be minimal for numbers around 1, and it would grows for high numbers and very small numbers (closer to 0) as it is shown in Figure \ref{fig:fp_err_model}. The only exception is zero, which can be encoded with a relative error equals to zero.

% ------------------------------------------------------------------------------
\subsection{Addition and multiplication error}
% ------------------------------------------------------------------------------

given that a relative error has the same definition for posit representation (Eq. \ref{eq:rel_err}) than floating point representation (Eq. \ref{eq:float_err_enc}), addition and multiplication error can be computed using the same formulas than those described for floating point in Equations \ref{eq:float_add_err} and \ref{eq:float_mult_err}.

The difference with floating point is that the actual value of the relative error $\epsilon$ is different given the number that is encoded. A summary of floating point and posit error bound model is shown in Figure \ref{fig:fp_err_model}. This shows that posit representation has a wider range that floating point numbers and that the error bound for posit may vary as a function of the numbers to be represented.

\begin{figure}[!ht]
	\centering
	\includestandalone{../Images/posit_error_model}
	\caption{Summary of the error bound model for positive posit and floating point numbers. The flat area for posit corresponds to numbers that can be encoded using a regime size of 2. For larger (or smaller) numbers, the regime size increases and the error increases as well because there are less bits to encode the fraction. Floating point has a constant relative error bound due to its fixed size fraction field.}
	\label{fig:fp_err_model}
\end{figure}


% ==============================================================================
\section{Error bound for SPNs}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Floating point}
% ------------------------------------------------------------------------------

In order to compute the error bound for a \gls{spn}, the error must be computed for each node and be propagated up to the output. The error bound that is computed must be the worst case scenario. Therefore, there should not be any zero values since that is the only exception that can be encoded with a zero error bound.

By setting all the literals to one, and propagating the error up to the output of the \gls{spn}, we can then compute the error bound of floating point representation for a given network.

In addition to this, an additional computation must be done to ensure that floating point can be used to represent every numbers in the \gls{spn} (even the smallest). Therefore the minimum output of the network must also be computed.

% ------------------------------------------------------------------------------
\subsection{Posit}
% ------------------------------------------------------------------------------
Error bound for posit require more work since the encoding error bound depends on the value which is encoded. Two cases must be taken into account: the maximum value of the \gls{spn}, and the minimum non-zero value of the \gls{spn}. Both of these can be the worst case scenario for a \gls{spn} since that are the values that require the most number of regime bits. Therefore there would be less bits allocated to the fraction and the relative error will be high.

\paragraph{Error for maximum output value}

This error can be computed by setting every input literal to one and propagate the error bounds up to the output as it was done for floating point. However, this error is rarely the biggest error since it is common that the maximum output value of the \gls{spn} is equal to one since it represents a probability\footnote{It is not a mandatory condition. If the maximum output of the \gls{spn} is not equal to one, the we should divide any results by the maximum value to get a probability.}

\paragraph{Error for minimum output value}

The smallest value is more difficult to compute since there are a lot of possible combination of inputs. Therefore, a trick is used to get an approximation of the minimum possible value of the \gls{spn}. This trick consist in setting every literal to one (as for the maximum value), and replacing every SUM node by MIN nodes. In this case, the output of the \gls{spn} may not be a valid set of literals. But the output is smaller or equal to the minimum output for a valid set of literals.

In the end, the posit error bound for an \gls{spn} is the maximum value between the relative error bound computed for maximum output value and minimum output value.